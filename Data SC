import requests
from bs4 import BeautifulSoup
import pymongo
from django_crontab import Crontab
# Create a MongoDB client
client = pymongo.MongoClient("mongodb://localhost:27017/")
# Create a new database
db = client["real_estate"]
# Create a collection within the database
collection = db["properties"]
# Define the function to scrape the property details from a single webpage
def scrape_property_details(url):
    # Send a GET request to the webpage
    response = requests.get(url)
    # Parse the HTML response
    soup = BeautifulSoup(response.text, "html.parser")
    # Extract the property details
    name = soup.find("h1", class_="Heading-StyledHeading-sc-10o2g5t-0").text
    cost = soup.find("div", class_="PriceCard__StyledPrice-sc-1454v74-0").text
    property_type = soup.find("div", class_="Heading-StyledHeading-sc-10o2g5t-0").text
    area = soup.find("div", class_="Heading-StyledHeading-sc-10o2g5t-0").text
    locality = soup.find("div", class_="Heading-StyledHeading-sc-10o2g5t-0").text
    city = soup.find("div", class_="Heading-StyledHeading-sc-10o2g5t-0").text
    property_link = url
    # Return the property details
    return {
        "name": name,
        "cost": cost,
        "property_type": property_type,
        "area": area,
        "locality": locality,
        "city": city,
        "property_link": property_link,
    }
# Define the function to schedule the scraping script to run twice daily
def schedule_scraping_script():
    # Create a crontab object
    crontab = Crontab()
    # Add a job to the crontab
    crontab.add_job(
        "python scrape_99acres.py",
        minute="0",
        hour="0,12",
        day_of_week="*",
        month_of_year="*",
         
    )
    # Start the crontab
    crontab.start()
# Scrape the property details from each webpage
for page_number in range(1, 10):
    url = f"https://www.99acres.com/property-in-bangalore-city?page={page_number}"
    response = requests.get(url)
    soup = BeautifulSoup(response.text, "html.parser")
    property_links = [
        a["href"] for a in soup.find_all("a", class_="Heading-StyledHeading-sc-10o2g5t-0")
    ]
    for property_link in property_links:
        property_details = scrape_property_details(property_link)
        collection.insert_one(property_details)
# Schedule the scraping script to run twice daily
schedule_scraping_script()
